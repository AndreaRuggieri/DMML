{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:54.095868500Z",
     "start_time": "2024-04-30T19:41:53.205318800Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://www.themoviedb.org/movie/'\n",
    "links = []\n",
    "for i in range(10000,10010):\n",
    "    links.append(link + str(i))\n",
    "len(links)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:54.097946500Z",
     "start_time": "2024-04-30T19:41:54.095360700Z"
    }
   },
   "id": "7d104c324823fcf8",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:54.132636Z",
     "start_time": "2024-04-30T19:41:54.099123700Z"
    }
   },
   "id": "f5ef926124b90c6f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.themoviedb.org/movie\", headers=headers)\n",
    "response.status_code\n",
    "dwn_content = response.text\n",
    "test_doc = BeautifulSoup(dwn_content, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:55.183568200Z",
     "start_time": "2024-04-30T19:41:54.102636200Z"
    }
   },
   "id": "afbcde2f04470aa8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'65'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_page_content(url):\n",
    "    # In this case , we are going to give request.get function headers to avoid the Status code Error 403\n",
    "\n",
    "    get_headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\"}\n",
    "    response_page = requests.get(url, headers = get_headers )\n",
    "    # we are going to raise exception here if status code gives any value other than 200.\n",
    "    if not response_page.ok:\n",
    "        raise Exception (\"Failed to request the data. Status Code:- {}\".format(response_page.status_code))\n",
    "    else:\n",
    "        page_content = response_page.text\n",
    "        doc_page = BeautifulSoup(page_content, \"html.parser\")\n",
    "        return doc_page\n",
    "popular_shows_url = \"https://www.themoviedb.org/movie\"\n",
    "doc = get_page_content(popular_shows_url)\n",
    "doc.title.text\n",
    "doc.find_all('div', {'class': 'card style_1'})[0].h2.text\n",
    "doc.find_all('div', {'class': 'user_score_chart'})[0]['data-percent']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.147008900Z",
     "start_time": "2024-04-30T19:41:55.188565100Z"
    }
   },
   "id": "d6efe2aefd70143f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def empty_dict():\n",
    "    scraped_dict = {  \n",
    "                    'Title': [], # ok\n",
    "                    'Genre': [],  # ok \n",
    "                    'Duration': [], # ok\n",
    "                    'Release_date': [], # ok\n",
    "                    'User_rating': [], # ok\n",
    "                    'Tagline': [], # ok\n",
    "                    'Description': [], # ok\n",
    "                    'Directors': [], # ok\n",
    "                    'Novel': [], # ok\n",
    "                    'Writers': [], # ok\n",
    "                    'Screenplay': [], # ok\n",
    "                    'State': [], \n",
    "                    'Language': [],\n",
    "                    'Budget': [],\n",
    "                    'Revenue': [],\n",
    "                    'Keywords': [],\n",
    "                    'Cast' : [] # ok\n",
    "                    }\n",
    "    return scraped_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.198905100Z",
     "start_time": "2024-04-30T19:41:56.146010300Z"
    }
   },
   "id": "1e643775d999bf2f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"def empty_dict():\\n    scraped_dict = {  \\n                    'Title': [],\\n                    'Genre': [],\\n                    'Release_date': [],\\n                    'User_rating': [],\\n                    'Tagline': [],\\n                    'Cast': []\\n                    }\\n    return scraped_dict\\n\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dict di prova!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\"\"\"def empty_dict():\n",
    "    scraped_dict = {  \n",
    "                    'Title': [],\n",
    "                    'Genre': [],\n",
    "                    'Release_date': [],\n",
    "                    'User_rating': [],\n",
    "                    'Tagline': [],\n",
    "                    'Cast': []\n",
    "                    }\n",
    "    return scraped_dict\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.226814400Z",
     "start_time": "2024-04-30T19:41:56.164438300Z"
    }
   },
   "id": "d1be3a5bcb6ca5be",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"def user_score_info(tag_user_score, i, scraped_dict):\n",
    "    if tag_user_score[i]['data-percent'] == '0':\n",
    "        scraped_dict['User_rating'].append('Not rated yet')\n",
    "    else:\n",
    "        scraped_dict['User_rating'].append(tag_user_score[i]['data-percent'])\"\"\"\n",
    "\n",
    "def user_score_info(tag_user_score, i):\n",
    "    if not tag_user_score[i] or 'data-percent' not in tag_user_score[i]:\n",
    "        return \"Not rated yet\"\n",
    "    else:\n",
    "        return tag_user_score[i]['data-percent']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.227837100Z",
     "start_time": "2024-04-30T19:41:56.183946300Z"
    }
   },
   "id": "398b551cf45a4ea1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'/movie/1096197-no-way-up'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find_all('div', {'class': 'card style_1'})[0].h2.a['href']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.229815900Z",
     "start_time": "2024-04-30T19:41:56.199902400Z"
    }
   },
   "id": "4f8fced661e09dbe",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_show_info(doc_page):\n",
    "    base_link_1 = \"https://www.themoviedb.org\"\n",
    "    tag_title = tag_premired_date = tag_shows_page = doc_page.find_all('div', {'class': 'card style_1'})\n",
    "    tag_user_score = doc_page.find_all('div', {\"user_score_chart\"}) \n",
    "    \n",
    "    doc_2_list = []\n",
    "    for link in tag_shows_page:\n",
    "        # here we are creating the list of all the individual pages of the shows which will come handy in other functions. \n",
    "        doc_2_list.append(get_page_content(\"https://www.themoviedb.org\" + link.h2.a['href']))\n",
    "       # we are going to have the function to return the list of all the information as elements. \n",
    "    return tag_title, tag_user_score, doc_2_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.230834500Z",
     "start_time": "2024-04-30T19:41:56.213300600Z"
    }
   },
   "id": "64d0c3333ad67068",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Commedia', 'Fantascienza']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets download and get the html of the individual page of the show 'what if...?' with the function get_page_content(). \n",
    "doc_2 = get_page_content(\"https://www.themoviedb.org/movie/10001\")\n",
    "tag_genre = doc_2.find('span', {\"class\": \"genres\"})\n",
    "tag_genre_list = tag_genre.find_all('a')\n",
    "\n",
    "check_genre =[]\n",
    "for tag in tag_genre_list:\n",
    "    check_genre.append(tag.text)\n",
    "\n",
    "check_genre"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.934571200Z",
     "start_time": "2024-04-30T19:41:56.217832Z"
    }
   },
   "id": "619fff9bb6ff1a2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# lets create a function to get the genres for the show. \n",
    "# i here denotes the element of the list vairable ``doc2_page`` that contains different doc pages. Will come handy later on.\n",
    "def get_genres(doc2_page, i):\n",
    "    genres_tags = doc2_page[i].find('span', {\"class\": \"genres\"}).find_all('a')\n",
    "    check_genre =[]\n",
    "    \n",
    "    for tag in genres_tags:\n",
    "        check_genre.append(tag.text)\n",
    "    return check_genre"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.946807300Z",
     "start_time": "2024-04-30T19:41:56.934571200Z"
    }
   },
   "id": "b6532aaf1a3dc235",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tag_tagline = doc_2.find('h3',{\"class\": 'tagline'})\n",
    "\n",
    "def tagline_info(doc_2_list, i, scraped_dict):\n",
    "    if doc_2_list[i].find('h3',{\"class\": 'tagline'}):\n",
    "        scraped_dict['Tagline'].append(doc_2_list[i].find('h3',{\"class\": 'tagline'}).text)\n",
    "    else:\n",
    "        scraped_dict['Tagline'].append(\"No Tagline\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.958183400Z",
     "start_time": "2024-04-30T19:41:56.940804800Z"
    }
   },
   "id": "177e76c9e24d082e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# i here denotes the the element of the list type variable``doc2_page`` that contains different doc pages.\n",
    "\n",
    "def get_show_cast(doc2_page, i):\n",
    "    cast_tags = doc2_page[i].find_all('li', {'class': 'card'})\n",
    "    cast_lis = []\n",
    "    \n",
    "    for t in cast_tags:\n",
    "         cast_lis.append(t.p.text)\n",
    "    \n",
    "    return cast_lis"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.959183700Z",
     "start_time": "2024-04-30T19:41:56.956174300Z"
    }
   },
   "id": "d973030db7968c4f",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_description(doc_2_list, i,scraped_dict): \n",
    "    if doc_2_list[i].find('div',{\"class\": 'overview'}):\n",
    "        scraped_dict['Description'].append(doc_2_list[i].find('div',{\"class\": 'overview'}).p.text)\n",
    "        # print(doc_2_list[i].find('div',{\"class\": 'overview'}).p.text)\n",
    "    else:\n",
    "        scraped_dict['Description'].append(\"No Description\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:56.990688900Z",
     "start_time": "2024-04-30T19:41:56.960182300Z"
    }
   },
   "id": "a560be6c70cb3caf",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_runtime_to_minutes(runtime):\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    \n",
    "    # Dividi la stringa per spazi per separare le parti ore e minuti\n",
    "    parts = runtime.split()\n",
    "    \n",
    "    for part in parts:\n",
    "        if 'h' in part:\n",
    "            # Estrai il numero di ore e convertilo in minuti\n",
    "            hours = int(part.replace('h', '')) * 60\n",
    "        elif 'm' in part:\n",
    "            # Estrai il numero di minuti\n",
    "            minutes = int(part.replace('m', ''))\n",
    "    \n",
    "    # Somma i minuti totali\n",
    "    total_minutes = hours + minutes\n",
    "    return total_minutes\n",
    "\n",
    "def get_runtime(doc_2_list, i, scraped_dict): \n",
    "    if doc_2_list[i].find('span',{\"class\": 'runtime'}):\n",
    "        scraped_dict['Runtime'].append(convert_runtime_to_minutes(doc_2_list[i].find('span',{\"class\":'runtime'}).text))\n",
    "        #print(convert_runtime_to_minutes(doc_2_list[i].find('span',{\"class\":'runtime'}).text))\n",
    "    else:\n",
    "        scraped_dict['Runtime'].append(\"No Runtime\")\n",
    "        # print(\"No runtime\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:57.258241500Z",
     "start_time": "2024-04-30T19:41:57.250244500Z"
    }
   },
   "id": "1612396583b5c3ef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_state(doc_2_list,i):\n",
    "    # Cerca il tag `<strong>` che contiene il testo \"Titolo originale\"\n",
    "    original_title_label = doc_2_list[i].find('section', class_='facts left_column')\n",
    "    p_tags_list = []\n",
    "\n",
    "    # Controlla se la sezione è stata trovata\n",
    "    if original_title_label:\n",
    "        # Trova tutti i tag <p> all'interno della sezione\n",
    "        p_tags = original_title_label.find_all('p')\n",
    "        # Aggiungi il testo di ogni tag <p> alla lista\n",
    "        for p_tag in p_tags:\n",
    "            p_tags_list.append(p_tag.get_text().strip()) \n",
    "        return parsing_state(p_tags_list)\n",
    "    else:\n",
    "        return  \"Stato non trovato\"\n",
    "\n",
    "def parsing_state(p_tags_list):\n",
    "    # Lista per i valori estratti\n",
    "    extracted_values = ['None','None','None','None','None']\n",
    "    \n",
    "    # Ciclo per estrarre i valori\n",
    "    for item in p_tags_list:\n",
    "        # Dividi la stringa una volta e prendi il secondo elemento\n",
    "        parts = item.split(' ', 1)\n",
    "        if len(parts) > 1:\n",
    "            type = parts[0]\n",
    "            value = parts[1]\n",
    "            if type == 'Stato':\n",
    "                extracted_values[1] = value\n",
    "            if type=='Lingua':\n",
    "                value = value.replace('Originale','')\n",
    "                extracted_values[2] = value\n",
    "            if type=='Titolo':\n",
    "                value = value.replace('originale','')\n",
    "                extracted_values[0] = value\n",
    "            if type=='Budget':\n",
    "                value = value.replace('$', '').replace(',', '').strip()\n",
    "                extracted_values[3] = value\n",
    "            if type=='Incasso':\n",
    "                value = value.replace('$', '').replace(',', '').strip()\n",
    "                extracted_values[4] = value\n",
    "    return extracted_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.253244300Z"
    }
   },
   "id": "e7722e66cc7c5216",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_directors(doc_2_list,i,scraped_dict): \n",
    "    profiles = doc_2_list[i].find('ol',{\"class\": 'people no_image'}).find_all('li')\n",
    "    for profile in profiles:\n",
    "        name = profile.find('a').text  # Trova il tag <a> e ottieni il testo\n",
    "        role = profile.find('p', class_='character').text  # Trova il <p> con classe \"character\" e ottieni il testo\n",
    "        if role == 'Director':\n",
    "            scraped_dict['Directors'].append(name)\n",
    "        elif role == 'Screenplay':\n",
    "            scraped_dict['Screenplay'].append(name)\n",
    "        elif role == 'Writer':\n",
    "            scraped_dict['Writers'].append(name)\n",
    "        elif role == 'Novel':\n",
    "            scraped_dict['Novel'].append(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.255240200Z"
    }
   },
   "id": "71a239ea607cbbb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_keywords(doc2_list,i, scraped_dict):\n",
    "    keywords_section = doc2_list[i].find('section', class_='keywords right_column')\n",
    "    keyword_lis = keywords_section.find_all('li')\n",
    "    keywords = [li.get_text(strip=True) for li in keyword_lis]\n",
    "    scraped_dict['keywords'] = keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.257241700Z"
    }
   },
   "id": "b2150ceffd6807b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fillna(scraped_dict):\n",
    "    print(scraped_dict)\n",
    "\n",
    "fillna(empty_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:57.267748900Z",
     "start_time": "2024-04-30T19:41:57.259241100Z"
    }
   },
   "id": "20118842de014951",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ensure_equal_length(scraped_dict):\n",
    "    max_length = max(len(lst) for lst in scraped_dict.values())  # Trova la lunghezza massima delle liste\n",
    "    for key, value_list in scraped_dict.items():\n",
    "        current_length = len(value_list)\n",
    "        if current_length < max_length:\n",
    "            # Riempi la lista con 'None' o con un altro valore di default fino a raggiungere la lunghezza massima\n",
    "            value_list.extend([None] * (max_length - current_length))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.260239800Z"
    }
   },
   "id": "93cb3b8d3b206023",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.262749600Z"
    }
   },
   "id": "2612421963c5eff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_show_details(t_title, t_user_score, docs_2_list):\n",
    "    # excuting a function here that empties the dictionary every time the function is called.\n",
    "    scraped_dict = {'Title': [], 'Genre': [], 'UserScore': [], 'Revenue': [], 'Cast': []}\n",
    "    for i in range (0, len(t_title)):\n",
    "        if t_title[i] is not None:\n",
    "            title_text = t_title[i].h2.text if t_title[i].h2 else \"Title Not Found\"\n",
    "            scraped_dict['Title'].append(title_text)\n",
    "        else:\n",
    "            scraped_dict['Title'].append(\"Title Not Found\")\n",
    "        # scraped_dict['Title'].append(t_title[i].h2.text)\n",
    "        scraped_dict['Genre'].append(get_genres(docs_2_list, i))\n",
    "        # user_score_info(t_user_score, i, scraped_dict)    \n",
    "        scraped_dict['User_rating'].append(user_score_info(t_user_score, i))\n",
    "        scraped_dict['Release_date'].append(t_title[i].p.text)\n",
    "        tagline_info(docs_2_list, i, scraped_dict)  \n",
    "        get_description(doc_2_list_,i,scraped_dict)\n",
    "        get_directors(docs_2_list,i,scraped_dict)\n",
    "        scraped_dict['State'].append(get_state(docs_2_list,i)[1])\n",
    "        scraped_dict['Language'].append(get_state(docs_2_list,i)[2])\n",
    "        scraped_dict['Budget'].append(get_state(docs_2_list,i)[3])\n",
    "        scraped_dict['Revenue'].append(get_state(docs_2_list,i)[4])\n",
    "        get_keywords(docs_2_list,i,scraped_dict)\n",
    "        scraped_dict['Cast'].append(get_show_cast(docs_2_list, i))\n",
    "        # Dopo aver popolato scraped_dict con i tuoi dati di scraping:\n",
    "        ensure_equal_length(scraped_dict)\n",
    "        df = pd.DataFrame(scraped_dict)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.264748400Z"
    }
   },
   "id": "d6f30762128328de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tag_title_, tag_user_score_, doc_2_list_ = get_show_info(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.265749100Z"
    }
   },
   "id": "ed66013dac254de",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's excute our function to check if it works. We are going to take a look the data of dataframe.\n",
    "\n",
    "x = get_show_details(tag_title_, tag_user_score_, doc_2_list_)\n",
    "x.to_csv('newcheck.csv')\n",
    "pd.read_csv('newcheck.csv',index_col=[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.267748900Z"
    }
   },
   "id": "14ed9871d4611ff3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_link = \"https://www.themoviedb.org/movie\"\n",
    "import os\n",
    "# 'i' here means the number of page we want to extract\n",
    "def create_page_df( i, dataframe_list):\n",
    "    os.makedirs('shows-data', exist_ok = True)\n",
    "    next_url = base_link + '?page={}'.format(i)\n",
    "    doc_top = get_page_content(next_url)\n",
    "    name_tag, viewer_score_tag, doc_2_lis = get_show_info(doc_top)\n",
    "    print('scraping page {} :- {}'.format(i, next_url))\n",
    "    dataframe_data = get_show_details(name_tag, viewer_score_tag, doc_2_lis)\n",
    "    dataframe_data.to_csv(\"shows-data/shows-page-{}.csv\".format(i) , index = None)\n",
    "    print(\" ---> a CSV file with name shows-page-{}.csv has been created\".format(i))\n",
    "    dataframe_list.append(dataframe_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T19:41:57.279746200Z",
     "start_time": "2024-04-30T19:41:57.268748200Z"
    }
   },
   "id": "f1e26278add94ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_list = []\n",
    "create_page_df(9 , test_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.269747600Z"
    }
   },
   "id": "4fce0296c330baae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_link = \"https://www.themoviedb.org/movie\"\n",
    "\n",
    "def scrape_top_200_shows(base_link):\n",
    "    dataframe_list = []\n",
    "    # we are going to keep range up to 11 because we just need up to 200 TV shows for now. \n",
    "    for i in range(1,11):\n",
    "        create_page_df(i, dataframe_list)\n",
    "    # here we are using concat function so that we can merge the each dataframe that we got from the each page.    \n",
    "    total_dataframe = pd.concat(dataframe_list, ignore_index = True)\n",
    "    \n",
    "    # with the simple command of to_csv() we can create a csv file of all the pages we extracted.\n",
    "    csv_complete =  total_dataframe.to_csv('shows-data/Total-dataframe.csv', index= None)\n",
    "    print(\" \\n a CSV file named Total-dataframe.csv with all the scraped shows has been created\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.270748500Z"
    }
   },
   "id": "68b16e16dafdf202",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scrape_top_200_shows(base_link)\n",
    "pd.read_csv('shows-data/Total-dataframe.csv')[0:50]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-30T19:41:57.272748400Z"
    }
   },
   "id": "535efb7bff1d75d1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "NO SAFE ZONE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "891325913cf3706b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
